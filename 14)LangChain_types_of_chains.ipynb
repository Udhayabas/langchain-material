{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b43d3f-608e-4f75-8ba6-510d838b6d0d",
   "metadata": {},
   "source": [
    "| **Chain / Retriever**               | **Returns Citations** | **Method / Output Key**                          | **Status**             | **Docs Link**                                                                               |\n",
    "| ----------------------------------- | --------------------- | ------------------------------------------------ | ---------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| `RetrievalQAWithSourcesChain`       | ‚úÖ Yes                 | `result['answer']`, `result['sources']`          | **Stable**             | [üîó Docs](https://docs.langchain.com/docs/modules/chains/popular/retrieval-qa-with-sources) |\n",
    "| `ConversationalRetrievalChain`      | ‚úÖ Yes                 | `result['answer']`, `result['source_documents']` | **Stable**             | [üîó Docs](https://docs.langchain.com/docs/modules/chains/popular/chat_vector_db)            |\n",
    "| `MultiRetrievalQAChain`             | ‚úÖ Yes                 | Depends on sub-chains used                       | **Stable**             | [üîó Docs](https://docs.langchain.com/docs/modules/chains/popular/multi_retrieval_qa)        |\n",
    "| `VectorDBQAWithSourcesChain`        | ‚úÖ Yes                 | `result['answer']`, `result['sources']`          | ‚úÖ (but legacy)         | [üîó Docs](https://docs.langchain.com/docs/modules/chains/popular/vector-db-qa)              |\n",
    "| `Tool` using RetrievalQAWithSources | ‚úÖ Yes                 | `result['answer']`, `result['sources']`          | **Stable** (via Agent) | [üîó Tools Docs](https://docs.langchain.com/docs/modules/agents/tools/custom_tools)          |\n",
    "| `RetrievalQA` (basic chain)         | ‚ùå No                  | Only `answer`                                    | **Stable**             | [üîó Docs](https://docs.langchain.com/docs/modules/chains/popular/retrieval-qa)              |\n",
    "| `RefineDocumentsChain`              | ‚ùå No                  | Only `answer`                                    | **Stable**             | [üîó Docs](https://docs.langchain.com/docs/modules/chains/document/refine)                   |\n",
    "| `StuffDocumentsChain`               | ‚ùå No                  | Only `answer`                                    | **Stable**             | [üîó Docs](https://docs.langchain.com/docs/modules/chains/document/stuff)                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70357767-bb85-4437-ab47-b7dc0a7639f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Step 1: Imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# ‚úÖ Step 2: Load API key\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ‚úÖ Step 3: Setup LLM and Embeddings\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# ‚úÖ Step 4: Load and split document\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=300, chunk_overlap=50)\n",
    "texts = splitter.split_text(raw_text)\n",
    "documents = [Document(page_content=t) for t in texts]\n",
    "\n",
    "# ‚úÖ Step 5: Vector Store (optional for Retriever chains)\n",
    "vectorstore = FAISS.from_texts(texts, embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"‚úÖ Base setup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0293d7-d339-48b4-913c-4b6bdd455115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14a6b75-a4bf-42aa-9c3c-32404a2681df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_6680\\2330394345.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_6680\\2330394345.py:15: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå StuffDocumentsChain Answer: The document is about LangChain, a framework created by Harrison Chase for building applications with LLMs. It discusses the features and uses of LangChain, such as supporting RAG, agents, memory, tools, and more, and its common applications in chatbots, document Q&A, and AI workflow.\n"
     ]
    }
   ],
   "source": [
    "#‚úÖ 4. StuffDocumentsChain\n",
    "#Use case: Combines all docs into a single string before passing to LLM (best for small number of documents).\n",
    "\n",
    "from langchain.chains import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# Define prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Use the following context to answer the question:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Inner LLM Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Stuff Chain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "# Run\n",
    "response = stuff_chain.invoke({\n",
    "    \"input_documents\": documents[:3],  # test on 3 docs\n",
    "    \"question\": \"What is this document about?\"\n",
    "})\n",
    "\n",
    "print(\"\\nüìå StuffDocumentsChain Answer:\", response[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d93fd-1860-4856-ad1e-0049c18e6851",
   "metadata": {},
   "source": [
    "# 5) Runnabale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f0aab87-f9b7-484d-a553-21ad9adb3f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Refined Summary:\n",
      "\n",
      "LangChain is a framework developed by Harrison Chase for building applications with LLMs. It supports various features such as RAG, agents, memory, and tools, and is commonly used in chatbots, document Q&A, and AI workflow.\n"
     ]
    }
   ],
   "source": [
    "# Initial prompt to summarize the first chunk\n",
    "#Use case: Starts with a base answer and refines it using subsequent documents ‚Äî good when each chunk contributes incrementally.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "initial_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Write a concise summary of the following text:\n",
    "\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "# Refine prompt to update the previous summary with new context\n",
    "refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "We have an existing summary:\n",
    "\"{existing_answer}\"\n",
    "\n",
    "Refine the summary with this new context:\n",
    "\"{context}\"\n",
    "\n",
    "If the context isn't useful, return the original summary.\n",
    "\"\"\")\n",
    "\n",
    "# Set up individual chains\n",
    "initial_summary_chain = initial_prompt | llm | StrOutputParser()\n",
    "refine_summary_chain = refine_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Start with first chunk\n",
    "summary = initial_summary_chain.invoke({\"context\": documents[0].page_content})\n",
    "\n",
    "# Iteratively refine with remaining docs\n",
    "for doc in documents[1:]:\n",
    "    summary = refine_summary_chain.invoke({\n",
    "        \"existing_answer\": summary,\n",
    "        \"context\": doc.page_content\n",
    "    })\n",
    "\n",
    "# Output final summary\n",
    "print(\"üìÑ Refined Summary:\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b4270-1b71-4f98-a24e-419056621652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
