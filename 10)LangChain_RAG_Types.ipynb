{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd14591-82bf-434f-ab35-4243fba16bb2",
   "metadata": {},
   "source": [
    "| **Retriever Type**                   | **Purpose**                                                                         | **Ideal Usage Scenario**                                                                                                       |\n",
    "| ------------------------------------ | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **VectorStoreRetriever**             | Retrieve documents based on vector similarity search (e.g. FAISS, Chroma, Pinecone) | Basic RAG (Retrieval-Augmented Generation) setup for fetching semantically relevant documents                                  |\n",
    "| **ContextualCompressionRetriever**   | Compresses retrieved docs using an LLM to return only the most relevant parts       | When using large context docs (PDFs, transcripts) where only a portion of each doc is relevant                                 |\n",
    "| **MultiQueryRetriever**              | Generates multiple rephrased queries to improve retrieval coverage                  | When a user query can be ambiguous or phrased differently; improves diversity of retrieved documents                           |\n",
    "| **ParentDocumentRetriever**          | Retrieves a full parent document instead of the split chunk                         | When a chunk is matched but full document context is required (e.g., blog, article, contract analysis)                         |\n",
    "| **BM25Retriever**                    | Keyword-based retriever using classic BM25 algorithm                                | When you want to retrieve based on exact keyword matches (useful for legal, medical, or short documents with precise language) |\n",
    "| **EnsembleRetriever**                | Combines multiple retrievers (e.g., vector + BM25) with weighted scoring            | When you want to merge semantic and keyword-based search for better hybrid performance                                         |\n",
    "| **TimeWeightedVectorStoreRetriever** | VectorStoreRetriever + time-based decay to prefer recent documents                  | Best for chatbot memory or news search where **recency** of information is important                                           |\n",
    "| **TavilySearchAPIRetriever**         | Uses external web search engine via Tavily API to retrieve **live web data**        | When querying real-world events, breaking news, or updated information outside your local docs or vector database              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934f2c5d-d07d-4ae3-b43a-62e522746402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_29848\\2968134909.py:20: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0)\n",
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_29848\\2968134909.py:31: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n",
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_29848\\2968134909.py:43: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_29848\\2968134909.py:59: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "C:\\Users\\Ramisha Aravind\\AppData\\Local\\Temp\\ipykernel_29848\\2968134909.py:70: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res1 = agent.run(\"What is LangChain?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ First question\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? Yes\n",
      "Action: RAG_QA\n",
      "Action Input: What is LangChain?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLangChain is a framework created by Harrison Chase for building applications with Large Language Models (LLMs). It supports various features such as RAG, agents, memory, tools, and more. LangChain is commonly used in applications like chatbots, document Q&A, and AI workflow.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: LangChain is a framework created by Harrison Chase for building applications with Large Language Models (LLMs). It supports various features such as RAG, agents, memory, tools, and more. LangChain is commonly used in applications like chatbots, document Q&A, and AI workflow.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: LangChain is a framework created by Harrison Chase for building applications with Large Language Models (LLMs). It supports various features such as RAG, agents, memory, tools, and more. LangChain is commonly used in applications like chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "2️⃣ Follow-up\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "AI: LangChain was created by Harrison Chase.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: LangChain was created by Harrison Chase.\n",
      "\n",
      "3️⃣ Ask again\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: `LangChain is a framework created by Harrison Chase for building applications with Large Language Models (LLMs). It supports features like RAG, agents, memory, tools, and more, and is commonly used in applications like chatbots, document Q&A, and AI workflow.`\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? Yes\n",
      "Action: RAG_QA\n",
      "Action Input: Explain LangChain simply\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLangChain is a framework created by Harrison Chase for building applications using Large Language Models (LLMs). It supports various features like RAG, agents, memory, tools, and more. LangChain is commonly used in applications such as chatbots, document question and answer systems, and AI workflow management.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: LangChain is a framework created by Harrison Chase for building applications using Large Language Models (LLMs). It supports various features like RAG, agents, memory, tools, and more. LangChain is commonly used in applications such as chatbots, document question and answer systems, and AI workflow management.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer: LangChain is a framework created by Harrison Chase for building applications using Large Language Models (LLMs). It supports various features like RAG, agents, memory, tools, and more. LangChain is commonly used in applications such as chatbots, document question and answer systems, and AI workflow management.\n"
     ]
    }
   ],
   "source": [
    "#1)VectorStoreRetriever\tVector-based\tEmbedding similarity search\tGeneral-purpose RAG\n",
    "# ConversationalRetrievalChain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "\n",
    "# 1. Setup\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 2️⃣ Load API keys\n",
    "load_dotenv(\".env\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 2. Vector DB\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# 🧠 Split the text into smaller chunks\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=300, chunk_overlap=50)\n",
    "texts = splitter.split_text(text_data)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(texts, embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 3. Conversational RAG chain\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "# 4. Memory for chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 5. Wrap RAG as a StructuredTool\n",
    "def rag_tool_fn(question: str) -> str:\n",
    "    return rag_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": []\n",
    "    })[\"answer\"]\n",
    "\n",
    "rag_tool = StructuredTool.from_function(\n",
    "    name=\"RAG_QA\",\n",
    "    description=\"Use this to answer questions about LangChain.\",\n",
    "    func=rag_tool_fn\n",
    ")\n",
    "\n",
    "# 6. Create agent\n",
    "agent = initialize_agent(\n",
    "    tools=[rag_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# 7. Run conversation\n",
    "print(\"1️⃣ First question\")\n",
    "res1 = agent.run(\"What is LangChain?\")\n",
    "print(\"Answer:\", res1)\n",
    "\n",
    "print(\"\\n2️⃣ Follow-up\")\n",
    "res2 = agent.run(\"Who created it?\")\n",
    "print(\"Answer:\", res2)\n",
    "\n",
    "print(\"\\n3️⃣ Ask again\")\n",
    "res3 = agent.run(\"Explain LangChain again simply.\")\n",
    "print(\"Answer:\", res3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42c743-d2ec-4ad8-99de-204d5b0df93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
