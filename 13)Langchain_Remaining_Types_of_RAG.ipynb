{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8cc5a1-2c9f-4256-8efc-a679003a3e9d",
   "metadata": {},
   "source": [
    "‚úÖ 4. ParentDocumentRetriever\n",
    "\n",
    "Use Case: Retrieve parent chunks based on a retrieval of smaller child chunks, great for keeping context intact in long documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6925d-5650-4590-bd80-8a7998a3e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 1. Load environment\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# 1. Prepare documents\n",
    "docs = [Document(page_content=\"LangChain helps build LLM-powered apps with memory and agents.\", metadata={\"id\": \"1\"}),\n",
    "        Document(page_content=\"Agents in LangChain use tools to answer questions.\", metadata={\"id\": \"2\"})]\n",
    "\n",
    "# 2. Setup child splitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# 3. Setup vectorstore for children\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "# 4. Parent retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=InMemoryStore(),  # Stores parent docs\n",
    "    child_splitter=child_splitter\n",
    ")\n",
    "\n",
    "# 5. Add documents\n",
    "retriever.add_documents(docs)\n",
    "\n",
    "# 6. Retrieve\n",
    "results = retriever.get_relevant_documents(\"What are agents?\")\n",
    "for doc in results:\n",
    "    print(\"üìÑ Retrieved Doc:\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29717e69-5ffd-47bb-a879-e2af523bdc76",
   "metadata": {},
   "source": [
    "‚úÖ 5. BM25Retriever\n",
    "\n",
    "Use Case: Pure keyword-based search (like traditional search engines). No embeddings needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc280cc7-a974-48af-91ce-7e8c67aa41d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\envs\\langchainhope\\lib\\site-packages (from rank_bm25) (2.2.6)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481cbdfa-cdef-4587-a8e6-c0bff403eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù BM25 Result: BM25 is a classical retrieval method.\n",
      "üìù BM25 Result: Vector search is powerful.\n",
      "üìù BM25 Result: LangChain enables LLM applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Create simple text docs\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain enables LLM applications.\"),\n",
    "    Document(page_content=\"Vector search is powerful.\"),\n",
    "    Document(page_content=\"BM25 is a classical retrieval method.\")\n",
    "]\n",
    "\n",
    "# Create BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# Retrieve\n",
    "results = bm25_retriever.get_relevant_documents(\"How does BM25 work?\")\n",
    "for doc in results:\n",
    "    print(\"üìù BM25 Result:\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e436933-001a-4a63-a9fc-d6e8b45207b1",
   "metadata": {},
   "source": [
    "‚úÖ 6. EnsembleRetriever\n",
    "\n",
    "Use Case: Combine multiple retrievers (e.g., keyword + vector-based) with weighted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a675d06a-2c9c-49d2-a279-9354dcda93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ensemble Doc: You can build AI apps using LangChain.\n",
      "üîç Ensemble Doc: LangChain supports LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Sample docs\n",
    "docs = [Document(page_content=\"LangChain supports LLMs.\"), Document(page_content=\"You can build AI apps using LangChain.\")]\n",
    "\n",
    "# BM25 Retriever\n",
    "bm25 = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# Vector Retriever\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Ensemble Retriever (equal weight)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25, vector_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = ensemble_retriever.get_relevant_documents(\"AI apps using LangChain\")\n",
    "for doc in results:\n",
    "    print(\"üîç Ensemble Doc:\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac49892-52c3-496a-9d44-cab63ae1bd12",
   "metadata": {},
   "source": [
    "7.TimeWeightedVectorStoreRetriever\n",
    "\n",
    "#This retriever boosts document relevance by factoring recency and importance of interactions (used in agents with memory or relevance ranking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c962678-359b-4304-8c14-bac43c1b5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.TimeWeightedVectorStoreRetriever\n",
    "#This retriever boosts document relevance by factoring recency and importance of interactions (used in agents with memory or relevance ranking).\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.schema import Document\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Initialize embedding & vectorstore\n",
    "embedding = OpenAIEmbeddings()\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is for LLM-based apps\", metadata={\"last_accessed_at\": datetime.now()}),\n",
    "    Document(page_content=\"Vector search improves relevance\", metadata={\"last_accessed_at\": datetime.now()})\n",
    "]\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "# TimeWeighted Retriever\n",
    "retriever = TimeWeightedVectorStoreRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    decay_rate=0.01,\n",
    "    k=2,\n",
    "    score_threshold=None\n",
    ")\n",
    "\n",
    "# Retrieve\n",
    "results = retriever.get_relevant_documents(\"What is LangChain?\")\n",
    "for r in results:\n",
    "    print(r)\n",
    "    print(r.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f39edcb-ff4b-4956-ac0f-57b82f724e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5aa949-c24f-4ced-aa97-e0737c41ba28",
   "metadata": {},
   "source": [
    "8.TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c81784a-e7c5-4efb-bc43-aa79043903f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain v0.3. Last updated: 09.16.24. What's changed. All packages have been upgraded from Pydantic 1 to Pydantic 2 internally.\n",
      "Connect traces in LangSmith to server logs in LangGraph Platform ¬∑ July 31, 2025 ; Introducing Align Evals: Streamlining LLM Application Evaluation ¬∑ July 29, 2025.\n",
      "What's new in LangChain? ¬∑ Better streaming support via the Event Streaming API. ¬∑ Standardized tool calling support ¬∑ A standardized interface for structuring\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import TavilySearchAPIRetriever\n",
    "import os\n",
    "\n",
    "# Set your Tavily API key\n",
    "#os.environ[\"TAVILY_API_KEY\"] = \"your_tavily_api_key\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# üîê Load API keys\n",
    "load_dotenv(\".env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Initialize Tavily Retriever\n",
    "retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "# Perform search\n",
    "docs = retriever.get_relevant_documents(\"Latest updates about LangChain\")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4957e8-0b32-443a-b64c-27bb3485be7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8950db-a066-4f2e-afa3-7d8a463e8ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab5590-36df-443a-a815-8913f55490b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
